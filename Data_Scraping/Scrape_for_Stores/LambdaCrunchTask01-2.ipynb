{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLmbOMNl2-JQ",
        "outputId": "ffd1b4de-779e-494b-ca8c-0744f8f373ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting firecrawl\n",
            "  Downloading firecrawl-1.6.4-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from firecrawl) (2.32.3)\n",
            "Collecting python-dotenv (from firecrawl)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.10/dist-packages (from firecrawl) (14.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from firecrawl) (1.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->firecrawl) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->firecrawl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->firecrawl) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->firecrawl) (2024.8.30)\n",
            "Downloading firecrawl-1.6.4-py3-none-any.whl (16 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv, firecrawl\n",
            "Successfully installed firecrawl-1.6.4 python-dotenv-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install firecrawl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pGYYsYS2XT6",
        "outputId": "2ffd3b2f-5d76-4ae6-aad0-61e7d69b48f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Starting to scrape 18 new URLs...\n",
            "Scraping completed and updated data appended to '/content/drive/My Drive/final_scrape_21.json'\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from firecrawl import FirecrawlApp\n",
        "from pydantic import BaseModel\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path where the JSON file will be saved in Drive\n",
        "final_output_path = '/content/drive/My Drive/final_scrape_21.json'\n",
        "\n",
        "# Initialize the FirecrawlApp with your API key\n",
        "app = FirecrawlApp(api_key='fc-f467709c7fb64865b23aa850076358a8')\n",
        "\n",
        "# Define the schema for the scraped data\n",
        "class StoreSchema(BaseModel):\n",
        "    store_name: str\n",
        "    store_description: str\n",
        "    store_location: str\n",
        "    store_email_address: str\n",
        "    store_phone_number: str\n",
        "    url: str\n",
        "\n",
        "# Function to truncate descriptions to 80 words\n",
        "def truncate_description(description, max_words=80):\n",
        "    words = description.split()\n",
        "    if len(words) > max_words:\n",
        "        return \" \".join(words[:max_words]) + \"...\"\n",
        "    return description\n",
        "\n",
        "# Function to scrape store data\n",
        "def scrape_store_data(url):\n",
        "    try:\n",
        "        # Use FirecrawlApp to scrape the URL\n",
        "        data = app.scrape_url(url, {\n",
        "            'formats': ['extract'],\n",
        "            'extract': {\n",
        "                'schema': StoreSchema.model_json_schema(),\n",
        "            }\n",
        "        })\n",
        "        extracted_data = data.get(\"extract\", {})\n",
        "\n",
        "        # Truncate the store description if present\n",
        "        if \"store_description\" in extracted_data and extracted_data[\"store_description\"]:\n",
        "            extracted_data[\"store_description\"] = truncate_description(extracted_data[\"store_description\"])\n",
        "\n",
        "        # Ensure URL is included\n",
        "        extracted_data[\"url\"] = url\n",
        "\n",
        "        # Return the modified extracted data\n",
        "        return extracted_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error while scraping {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Your list of URLs goes here\n",
        "urls = [\n",
        "\n",
        "]\n",
        "\n",
        "unique_urls = list(set(urls))\n",
        "\n",
        "# If the file already exists and contains data, load it first\n",
        "scraped_data = []\n",
        "existing_urls = set()\n",
        "\n",
        "if os.path.exists(final_output_path) and os.path.getsize(final_output_path) > 0:\n",
        "    with open(final_output_path, 'r') as f:\n",
        "        try:\n",
        "            existing_data = json.load(f)\n",
        "            # Ensure it's a list\n",
        "            if isinstance(existing_data, list):\n",
        "                scraped_data = existing_data\n",
        "                # Collect existing URLs to prevent duplicates\n",
        "                existing_urls = {item.get(\"url\") for item in existing_data if \"url\" in item}\n",
        "        except json.JSONDecodeError:\n",
        "            # If the existing file is not valid JSON, we start fresh\n",
        "            scraped_data = []\n",
        "            existing_urls = set()\n",
        "\n",
        "# Filter out URLs that have already been scraped\n",
        "new_urls = [url for url in unique_urls if url not in existing_urls]\n",
        "\n",
        "if not new_urls:\n",
        "    print(\"No new URLs to scrape. All URLs have already been processed.\")\n",
        "else:\n",
        "    print(f\"Starting to scrape {len(new_urls)} new URLs...\")\n",
        "\n",
        "    # Scrape data from each new URL and append to the list\n",
        "    for url in new_urls:\n",
        "        store_data = scrape_store_data(url)\n",
        "        if store_data:\n",
        "            # Append the data to our list\n",
        "            scraped_data.append({\n",
        "                \"url\": store_data.get(\"url\", \"\"),\n",
        "                \"store_name\": store_data.get(\"store_name\", \"\"),\n",
        "                \"store_description\": store_data.get(\"store_description\", \"\"),\n",
        "                \"store_location\": store_data.get(\"store_location\", \"\"),\n",
        "                \"store_phone_number\": store_data.get(\"store_phone_number\", \"\")\n",
        "            })\n",
        "\n",
        "    # Write the updated scraped data list to the JSON file\n",
        "    with open(final_output_path, \"w\") as f:\n",
        "        json.dump(scraped_data, f, indent=4)\n",
        "\n",
        "    print(f\"Scraping completed and updated data appended to '{final_output_path}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRRf0iPq25Xl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5B0APetaoMFh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWEiVdOSqRVu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}